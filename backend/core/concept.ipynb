{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f6d9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6853b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from modules.mask_utils import Masker\n",
    "import bpy\n",
    "import mathutils\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FFMpegWriter, FuncAnimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca3c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(\n",
    "    os.path.join(\n",
    "        os.path.dirname(os.path.abspath('__file__')),\n",
    "        os.pardir,\n",
    "        os.pardir,\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd968ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend.core.modules.data_objects import CameraData, VideoData, HumanPose, Body, ContactPoints\n",
    "from backend.core.modules.plot_utils import coords_converter\n",
    "from backend.core.modules.cam_utils import get_extrinsic_matrix\n",
    "from backend.core.modules.body_keypoints import get_edge_coords_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae477198",
   "metadata": {},
   "source": [
    "# 3D Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0102233",
   "metadata": {},
   "source": [
    "Camera Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966f5518",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_json = {\n",
    "    \"fx\": 1353.6915950245486,\n",
    "    \"fy\": 1357.5577115711324,\n",
    "    \"cx\": 538.9620188123179,\n",
    "    \"cy\": 961.5329984766928,\n",
    "    \"dist_coeffs\": [\n",
    "        0.045870044322073686,\n",
    "        -0.39564683084494173,\n",
    "        -0.0020515588805692024,\n",
    "        0.0015103837799576112,\n",
    "        1.0718061310340865\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2891d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "cameradata = CameraData(cam_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b026e523",
   "metadata": {},
   "source": [
    "Redndering Camera Orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a5cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polar2cartesian_xz(r, theta):\n",
    "    \"\"\"\n",
    "    Get camera cartesian coordinates (only on X-Z plane) fropm ploar representation\n",
    "    r: distance from the global origin [m]\n",
    "    theta: angle [deg]. +z to +x axis is +theta.\n",
    "    \"\"\"\n",
    "    theta = np.deg2rad(theta)\n",
    "    x = r * np.sin(theta)\n",
    "    z = r * np.cos(theta)\n",
    "    return x, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f412ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cam_position2tvec(r, theta, height, vector_fmt=False):\n",
    "    \"\"\"\n",
    "    Obtain cartesian position vector from the cylindical coordinates to \n",
    "    \"\"\"\n",
    "    x, z = polar2cartesian_xz(r, theta)\n",
    "    y = height\n",
    "    if vector_fmt:\n",
    "        return np.array([[x], [y], [z]])\n",
    "    else:\n",
    "        return x, y, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39cae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cam_position2rvec(theta):\n",
    "    return np.array([\n",
    "        [np.deg2rad(0)],\n",
    "        [np.deg2rad(theta)], \n",
    "        [np.deg2rad(0)]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddcc50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_camera_vec(cam, tvec: np.ndarray, rvec: np.ndarray, name: str):\n",
    "    \"\"\"\n",
    "    cam: bpy.data.cameras\n",
    "    tvec: translation vector\n",
    "    rvec: rotation vector\n",
    "    \"\"\"\n",
    "    cam_obj = bpy.data.objects.new(name, cam)\n",
    "    cam_obj.location = (tvec[0][0], tvec[1][0], tvec[2][0])\n",
    "    cam_obj.rotation_mode = 'XYZ'\n",
    "    r = R.from_rotvec([rvec[0][0], rvec[1][0], rvec[2][0]])\n",
    "    cam_obj.rotation_euler = r.as_euler('xyz', degrees=False)\n",
    "    return cam_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0cd7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_camera(cam, r: float, theta: float, height: float, name: str):\n",
    "    \"\"\"\n",
    "    cam: bpy.data.cameras\n",
    "    r: Distance from the wall\n",
    "    theta: Angle of the camera measured from the centre line\n",
    "    height: Height of the camera\n",
    "    \"\"\"\n",
    "    cam_obj = bpy.data.objects.new(name, cam)\n",
    "    x, y, z = cam_position2tvec(r, theta, height)\n",
    "    cam_obj.location = (x, y, z)\n",
    "    cam_obj.rotation_mode = 'XYZ'\n",
    "    cam_obj.rotation_euler = (np.deg2rad(0), np.deg2rad(theta), np.deg2rad(0))\n",
    "    return cam_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5087ad8",
   "metadata": {},
   "source": [
    "Bleander camera object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10adf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_camera(f_in_mm, scene):\n",
    "    \"\"\"\n",
    "    scene: blender scene\n",
    "    Returns: bpy.data.cameras\n",
    "    \"\"\"\n",
    "    cam = bpy.data.cameras.new('Camera')\n",
    "    cam.lens = f_in_mm\n",
    "    sensor_w_in_mm = cam.sensor_width\n",
    "    res_x = scene.render.resolution_x\n",
    "    res_y = scene.render.resolution_y\n",
    "    scale = scene.render.resolution_percentage / 100.0\n",
    "    aspect_ratio = scene.render.pixel_aspect_y / scene.render.pixel_aspect_x\n",
    "    # Get pixel size\n",
    "    resolution_x_in_px = res_x * scale\n",
    "    resolution_y_in_px = res_y * scale\n",
    "    # Focal length in pixels\n",
    "    fx = f_in_mm / sensor_w_in_mm * resolution_x_in_px\n",
    "    fy = fx / aspect_ratio\n",
    "    # Principal point\n",
    "    cx = resolution_x_in_px / 2 - cam.shift_x * resolution_x_in_px\n",
    "    cy = resolution_y_in_px / 2 + cam.shift_y * resolution_y_in_px\n",
    "    # Construct K\n",
    "    K = np.array([\n",
    "        [fx, 0, cx],\n",
    "        [0, fy, cy],\n",
    "        [0,  0,  1]\n",
    "    ])\n",
    "    return cam, K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2911c648",
   "metadata": {},
   "source": [
    "Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c087242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(camera, save_to):\n",
    "    \"\"\"\n",
    "    camera: camera object\n",
    "    \"\"\"\n",
    "    # Render from cam0\n",
    "    scn.camera = camera\n",
    "    bpy.ops.render.render(write_still=True)\n",
    "    bpy.data.images[\"Render Result\"].save_render(filepath=save_to)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace0e7d",
   "metadata": {},
   "source": [
    "Blender Creating the Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff4192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deselect all\n",
    "bpy.ops.object.select_all(action='SELECT')\n",
    "bpy.ops.object.delete(use_global=False)\n",
    "\n",
    "# Create and activate scene\n",
    "new_scene = bpy.data.scenes.new(\"EmptyScene\")\n",
    "bpy.context.window.scene = new_scene\n",
    "scn = new_scene\n",
    "scn.unit_settings.system = 'METRIC'\n",
    "scn.unit_settings.scale_length = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723e992",
   "metadata": {},
   "source": [
    "Load the 3D mesh model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4640b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/susum/Desktop/MB2024.blend'\n",
    "with bpy.data.libraries.load(file_path, link=False, relative=True) as (data_from, data_to):\n",
    "    # data_from: data blocks available to link/append\n",
    "    # data_to: data blocks I want to load\n",
    "    data_to.objects = ['Mesh_0_Part_0']\n",
    "\n",
    "for obj in data_to.objects:\n",
    "    if obj is not None:\n",
    "        scn.collection.objects.link(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c93ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a point light\n",
    "light_data = bpy.data.lights.new(name='dataLight', type='POINT')\n",
    "light_data.energy = 800\n",
    "# Create a light object\n",
    "light = bpy.data.objects.new(name='Light', object_data=light_data)\n",
    "x, z = polar2cartesian_xz(7, 0)\n",
    "y = 2\n",
    "light.location = (x, y, z)\n",
    "scn.collection.objects.link(light)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9756a984",
   "metadata": {},
   "source": [
    "Creating Rendering Cameras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec06e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_in_mm = (camera_json['fx'] + camera_json['fy']) / 2\n",
    "f_in_mm = 35\n",
    "cam, K_blender = create_camera(f_in_mm, scn)\n",
    "cam_obj0 = add_camera(cam, 8, 0, 1.7, 'pov0')\n",
    "render(cam_obj0, 'C:/Users/susum/Desktop/MB2024_render0.png')\n",
    "cam_obj1 = add_camera(cam, 8, 30, 1.7, 'pov1')\n",
    "render(cam_obj1, 'C:/Users/susum/Desktop/MB2024_render1.png')\n",
    "cam_obj2 = add_camera(cam, 8, -30, 1.7, 'pov2')\n",
    "render(cam_obj2, 'C:/Users/susum/Desktop/MB2024_render2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aa7f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('C:/Users/susum/Desktop/MB2024_render0.png')\n",
    "# img1 = cv2.imread('C:/Users/susum/Desktop/MB2024_render1.png')\n",
    "# img1 = cv2.imread('C:/Users/susum/Desktop/MB2024_render2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc4ff85",
   "metadata": {},
   "source": [
    "Loading the video frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db98b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "videodata = VideoData(\"C:/Users/susum/Desktop/1StarChoss_trimmed.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4802d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "videodata_copy = VideoData(\"C:/Users/susum/Desktop/1StarChoss_trimmed.mp4\")\n",
    "human_pose = videodata_copy.get_human_pose()\n",
    "del videodata_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236708ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_number = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31cb107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img2 = cv2.imread('C:/Users/susum/Desktop/MB2024_record1.png')\n",
    "# img2 = cv2.imread('C:/Users/susum/Desktop/MB2024_record2.png')\n",
    "# img2 = cv2.imread('C:/Users/susum/Desktop/MB2024_record3.png')\n",
    "# img2 = cv2.imread('C:/Users/susum/Desktop/MB2024_record4.png')\n",
    "# img2 = cv2.imread('C:/Users/susum/Desktop/MB2024_record5.png')\n",
    "img2 = cv2.undistort(videodata.load_frame(frame_number), cameradata.intrinsic_matrix, cameradata.distortion_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64c1c41",
   "metadata": {},
   "source": [
    "Image Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gammaCorrection(src):\n",
    "    #  ratio of the log(mid)/log(mean)\n",
    "    mid = 0.3\n",
    "    mean = np.mean(src)\n",
    "    gamma = np.log(mid * 255) / np.log(mean)\n",
    "\n",
    "    invGamma = 1 / gamma\n",
    "\n",
    "    table = [((i / 255) ** invGamma) * 255 for i in range(256)]\n",
    "    table = np.array(table, np.uint8)\n",
    "\n",
    "    return cv2.LUT(src, table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6e6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert(src):\n",
    "    \"\"\"\n",
    "    Inverted grey image\n",
    "    \"\"\"\n",
    "    return cv2.bitwise_not(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c3cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colour -> Grey\n",
    "\n",
    "img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f2e4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blurred Grey Images\n",
    "\n",
    "# img1_gray = cv2.GaussianBlur(img1_gray, (3,3), cv2.BORDER_DEFAULT)\n",
    "img2_gray = cv2.GaussianBlur(img2_gray, (3,3), cv2.BORDER_DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma Correction\n",
    "\n",
    "# img1_adjust = img1_gray\n",
    "# img1_adjust = cv2.equalizeHist(img1_gray)\n",
    "img1_adjust = gammaCorrection(img1_gray)\n",
    "# img1_adjust = cv2.Canny(img1_adjust, 20, 20)\n",
    "\n",
    "\n",
    "# img2_adjust = img2_gray\n",
    "# img2_adjust = cv2.equalizeHist(img2_gray)\n",
    "img2_adjust = gammaCorrection(img2_gray)\n",
    "# img2_adjust = cv2.Canny(img2_adjust, 20, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e88dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert the Grey \n",
    "\n",
    "img1_ = invert(img1_adjust)\n",
    "img2_ = invert(img2_adjust)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f9959c",
   "metadata": {},
   "source": [
    "Matching Point Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13940ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maks the non-wall area out of the interest for the feature point detection\n",
    "yolomodel = 'models/best.pt'\n",
    "masker = Masker(yolomodel)\n",
    "mask_segment = masker.apply_mask(img2, 0.3, cls2include=[0], cls2exclude=[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac86924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A-KAZE detector\n",
    "akaze = cv2.AKAZE_create()\n",
    "# Initiate ORB detector\n",
    "# akaze = cv2.ORB_create()\n",
    "\n",
    "# Feature detection and feature vector computation\n",
    "# https://docs.opencv.org/4.x/d0/d13/classcv_1_1Feature2D.html#a8be0d1c20b08eb867184b8d74c15a677\n",
    "kp1, des1 = akaze.detectAndCompute(img1_, None)\n",
    "kp2, des2 = akaze.detectAndCompute(img2_, mask_segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ca953",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_check = True\n",
    "# cross_check = False\n",
    "\n",
    "# creating Brute-Force Matcher\n",
    "# https://docs.opencv.org/4.x/dc/dc3/tutorial_py_matcher.html\n",
    "if cross_check:\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING2, crossCheck=True)\n",
    "    matches = bf.match(des1, des2)\n",
    "    # Sort them in the order of their distance.\n",
    "    matches_good = sorted(matches, key = lambda x:x.distance)\n",
    "else:\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "    # Select pairs by similarity in feature vectors\n",
    "    matches_good = []\n",
    "    ratio = 0.5\n",
    "    while len(matches_good) < 20:\n",
    "        ratio += 0.01\n",
    "        matches_good = []\n",
    "        for m, n in matches:\n",
    "            if m.distance < ratio * n.distance:\n",
    "                matches_good.append([m])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646f1947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geometric Verification -> Assuming that the feature points are alighned on the same plane!\n",
    "# queryIdx: index of the descriptor in the first set\n",
    "# trainIdx: index of the descriptor in the second set\n",
    "\n",
    "# Rendered image\n",
    "pt1 = np.float32([kp1[m.queryIdx].pt for m in matches_good])\n",
    "# Snapshot\n",
    "pt2 = np.float32([kp2[m.trainIdx].pt for m in matches_good])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a836eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute homography using RANSAC\n",
    "# https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#ga4abc2ece9fab9398f2e560d53c8c9780\n",
    "# https://docs.opencv.org/4.x/d9/dab/tutorial_homography.html\n",
    "perspective_transformation, mask = cv2.findHomography(pt1, pt2, cv2.RANSAC, 5.0)\n",
    "\n",
    "matches_plane = []\n",
    "for i, m in enumerate(matches_good):\n",
    "    if mask[i]:\n",
    "        matches_plane.append(m)\n",
    "\n",
    "img3 = cv2.drawMatches(img1, kp1, img2, kp2, matches_plane, None, flags=2)\n",
    "cv2.imwrite('C:/Users/susum/Desktop/MB2024_matching.png', img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1ec315",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img3)\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4906b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(matches_plane)/len(matches_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd53c87d",
   "metadata": {},
   "source": [
    "Find 3D Coordinates of the Matching Points on the 3D Mesh Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b84472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_to_world_ray(x, y, cam_obj, cam_intrinsics, render_size):\n",
    "    \"\"\"\n",
    "    Calculate the ray vector in the camera direction originated from a point on the rendered image\n",
    "    \"\"\"\n",
    "    fx = cam_intrinsics[0,0]\n",
    "    fy = cam_intrinsics[1,1]\n",
    "    cx = cam_intrinsics[0,2]\n",
    "    cy = cam_intrinsics[1,2]\n",
    "    # fx, fy, cx, cy = cam_intrinsics[\"fx\"], cam_intrinsics[\"fy\"], cam_intrinsics[\"cx\"], cam_intrinsics[\"cy\"]\n",
    "    width, height = render_size\n",
    "\n",
    "    # Convert to normalized device coords\n",
    "    x_ndc = (x - cx) / fx\n",
    "    y_ndc = (y - cy) / fy\n",
    "    ray_camera = mathutils.Vector((x_ndc, -y_ndc, -1)).normalized()  # Blender uses -Z forward\n",
    "    # ray_camera = mathutils.Vector((x_ndc, y_ndc, 1)).normalized()\n",
    "\n",
    "    # Transform ray to world coordinates\n",
    "    ray_world = cam_obj.matrix_world.to_3x3() @ ray_camera\n",
    "    origin_world = cam_obj.matrix_world.to_translation()\n",
    "\n",
    "    return origin_world, ray_world.normalized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb63554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect_ray_with_mesh(scene, origin, direction):\n",
    "    \"\"\"\n",
    "    Coordinates of intersection with the rays from the feature points and the 3d mesh\n",
    "    \"\"\"\n",
    "    depsgraph = bpy.context.evaluated_depsgraph_get()\n",
    "    result, location, normal, index, obj, matrix = scene.ray_cast(\n",
    "        depsgraph, origin, direction\n",
    "    )\n",
    "    return location if result else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eebaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinates of the keypoints in reference and target images\n",
    "pt1 = np.float32([kp1[m.queryIdx].pt for m in matches_plane])\n",
    "pt2 = np.float32([kp2[m.trainIdx].pt for m in matches_plane])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91145b34",
   "metadata": {},
   "source": [
    "Compute the mesh & ray intersection points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b6157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth_ray_casting(coords2d, scene, cam_obj, K_blender):\n",
    "    \"\"\"\n",
    "    coords2d: [[x, y], ...]\n",
    "    scene: blender scene\n",
    "    cam_obj: blender camera object\n",
    "    K_blender: blender camera intrinsic matrix\n",
    "    \"\"\"\n",
    "    render_size = (scene.render.resolution_x, scene.render.resolution_y)\n",
    "\n",
    "    coords3d = []\n",
    "    for pt in coords2d:\n",
    "        origin, direction = pixel_to_world_ray(pt[0], pt[1], cam_obj, K_blender, render_size)\n",
    "        intersection = intersect_ray_with_mesh(scene, origin, direction)\n",
    "        if intersection is not None:\n",
    "            coords3d.append(intersection)\n",
    "        else:\n",
    "            coords3d.append(None)  # No hit\n",
    "    return coords3d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca99a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_2d_3d(pt_render, pt_videoframe, scene, cam_obj, K_blender):\n",
    "    \"\"\"\n",
    "    Get 2d and 3d coordinates identified from the feature matching\n",
    "    pt_render: 2d coordinates array from the mesh model. result of the feature mathcing\n",
    "    pt_videoframe: 2d coordinates array from the video frame snapshot.\n",
    "    \"\"\"\n",
    "    pt_ray_casting = get_depth_ray_casting(pt_render, scene, cam_obj, K_blender)\n",
    "    pts_wld = []\n",
    "    pts_img = []\n",
    "    for coords_w, coords_i in zip(pt_ray_casting, pt_videoframe):\n",
    "        if coords_w is not None:\n",
    "            pts_wld.append([coords_w[0], coords_w[1], coords_w[2]])\n",
    "            pts_img.append([coords_i[0], coords_i[1]])\n",
    "\n",
    "    # blender coords -> opencv coords\n",
    "    # pts_wld = np.array([coords_converter(coords, 'blender-opencv') for coords in pts_wld])\n",
    "    pts_wld = np.array([coords_converter(coords, 'blender-matplotlib') for coords in pts_wld])\n",
    "    pts_img = np.array(pts_img)\n",
    "    return pts_img, pts_wld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e5117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_img, pts_wld = get_pair_2d_3d(pt1, pt2, scn, cam_obj0, K_blender)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c075cc2",
   "metadata": {},
   "source": [
    "Visualise the 3D Coords of the feature points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b860ab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.set_aspect('equal')\n",
    "for coords in pts_wld:\n",
    "    x, y, z = coords\n",
    "    ax.scatter(x, y, z, c='black', s=3)\n",
    "ax.view_init(elev=60, azim=164, roll=-104)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6eb3cf",
   "metadata": {},
   "source": [
    "Computing the rotation & translation vectors for the 3D corrds to the 2D coords projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b37f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solvePnP(world_coords, img_coords, cameradata:CameraData, **kwargs):\n",
    "    \"\"\"\n",
    "    PnP solver for the wall and human pose\n",
    "    \"\"\"\n",
    "    vervose = kwargs.pop('vervose', False)\n",
    "    success, rvec, tvec, inliers = cv2.solvePnPRansac(\n",
    "        world_coords,\n",
    "        img_coords,\n",
    "        cameradata.intrinsic_matrix,\n",
    "        np.array(cam_json['dist_coeffs']),\n",
    "        flags=cv2.SOLVEPNP_ITERATIVE,\n",
    "        reprojectionError=8.0,\n",
    "        confidence=0.99\n",
    "    )\n",
    "\n",
    "    if success:\n",
    "        if vervose:\n",
    "            print(\"Rotation Vector:\\n\", rvec)\n",
    "            print(\"Translation Vector:\\n\", tvec)\n",
    "        return rvec, tvec\n",
    "    else:\n",
    "        print(\"PnP RANSAC failed.\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6f146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvec_wall, tvec_wall = solvePnP(pts_wld, pts_img, cameradata)\n",
    "rvec_wall, tvec_wall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c9ae9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # f_in_mm = (cam_json['fx'] + cam_json['fy']) / 2\n",
    "# f_in_mm = 35\n",
    "# cam_obs, K_blender = create_camera(f_in_mm, scn)\n",
    "# cam_obj = add_camera_vec(cam_obs, tvec_wall, rvec_wall, 'pov')\n",
    "# render(cam_obj, 'C:/Users/susum/Desktop/MB2024_render.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61ef793",
   "metadata": {},
   "outputs": [],
   "source": [
    "r, theta, height = 8, 0, 1.7\n",
    "rvec_render = cam_position2rvec(theta)\n",
    "tvec_render = cam_position2tvec(r, theta, height, vector_fmt=True)\n",
    "T_wall_rendering = get_extrinsic_matrix(rvec_render, tvec_render)\n",
    "T_wall_rendering_inv = np.linalg.inv(T_wall_rendering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced4d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_wall = get_extrinsic_matrix(rvec_wall, tvec_wall)\n",
    "T_wall_inv = np.linalg.inv(T_wall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70ead60",
   "metadata": {},
   "source": [
    "# Later work ...\n",
    " !Theoretical configuration of holds. Need to kow the exact rvec and tvec to the rendered image in blender to make it work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ace2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import os\n",
    "# class Holds:\n",
    "#     \"\"\"\n",
    "#     Get coordinates of all the holds projected to the video frame from the rotayion & translation matrix and the 3d coordinates of the holds\n",
    "#     coords_world: 3d coordinates of all the holds used for solving PnP\n",
    "#     rvec: Rotation vector\n",
    "#     tvec: Translation vector\n",
    "#     camera: CameraData instance\n",
    "#     \"\"\"\n",
    "#     def __init__(self, **kwargs):\n",
    "#         # wold coordinates of the holds\n",
    "#         self.path_coords_holds = os.path.join('C:/Users/susum/Desktop', 'world_coords_holds.pkl')\n",
    "#         self.cols = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K']\n",
    "#         self.col_colours = ['brown', 'red', 'moccasin', 'orange', 'yellow', 'green', 'darkcyan', 'blue', 'blueviolet', 'purple', 'pink']\n",
    "#         self.colour_dict = {k: c for k, c in zip(self.cols, self.col_colours)}\n",
    "\n",
    "#         if not os.path.exists(self.path_coords_holds):\n",
    "#             self.init_nodes(**kwargs)\n",
    "#         else:\n",
    "#             self.coords_world = pickle.load(open(self.path_coords_holds, 'rb'))\n",
    "#         self.rvec = kwargs.pop('rvec', None)\n",
    "#         self.tvec = kwargs.pop('tvec', None)\n",
    "#         self.camera = kwargs.pop('camera', None)  # CameraData instance\n",
    "#         self.K = self.camera.intrinsic_matrix\n",
    "#         self.dist = self.camera.distortion_coeff\n",
    "\n",
    "#     def init_nodes(self, **kwargs):\n",
    "#         \"\"\"\n",
    "#         Inititialise the node attributes\n",
    "#         \"\"\"\n",
    "#         self.coords_world = {}\n",
    "#         inverty = kwargs.pop('inverty', False)\n",
    "#         self.wall_angle = 40 # degrees\n",
    "#         theta = self.wall_angle / 180 * np.pi\n",
    "\n",
    "#         y0 = 100 * np.cos(theta) + 370\n",
    "#         z0 = 100 * np.sin(theta)\n",
    "#         for col in self.cols:\n",
    "#             for row in range(1,19):\n",
    "#                 # Centred on column F\n",
    "#                 xx = (self.cols.index(col) - 5) * 200\n",
    "#                 yy = y0 + 200 * np.cos(theta) * (row - 1)\n",
    "#                 # zz = z0 + 200 * np.sin(theta) * (row - 1)\n",
    "#                 zz = -(z0 + 200 * np.sin(theta) * (row - 1))\n",
    "#                 if inverty:\n",
    "#                     yy = -1 * yy\n",
    "#                 # self.coords_world[col + str(row)] = (xx, yy, zz)\n",
    "#                 self.coords_world[col + str(row)] = (xx*1e-3, yy*1e-3, zz*1e-3)  # mm to m\n",
    "\n",
    "#         pickle.dump(self.coords_world, open(self.path_coords_holds, 'wb'))\n",
    "\n",
    "#     def get_projection(self):\n",
    "#         \"\"\"\n",
    "#         Get the projection of the holds world coordinates to the video perspective\n",
    "#         \"\"\"\n",
    "#         # Map the 3D point to 2D point\n",
    "#         world_coords = np.array(list(self.coords_world.values())).astype('float32')\n",
    "#         coords_2d, _ = cv2.projectPoints(world_coords, \n",
    "#                                         self.rvec, self.tvec, \n",
    "#                                         self.K, \n",
    "#                                         self.dist)\n",
    "#         self.coords_frame = {k: v for k, v in zip(self.coords_world.keys(), coords_2d)}\n",
    "#         return self.coords_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kwargs = {\n",
    "#     'inverty': True,\n",
    "#     'rvec': rvec_wall,\n",
    "#     'tvec': tvec_wall, \n",
    "#     'camera': cameradata,\n",
    "# }\n",
    "# holds = Holds(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1435f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# # ax.imshow(img2)\n",
    "# ax.set_aspect('equal')\n",
    "# for k, v in holds.coords_world.items():\n",
    "#     xx, yy, zz, _ = T_wall @ T_wall_rendering @ [v[0], v[1], v[2], 1]\n",
    "#     # xxx, yyy, zzz, _ = T_wall_inv @ [xx, yy, zz, 1]\n",
    "#     # ax.scatter(xxx, yyy, zzz, label=k, s=1)\n",
    "#     ax.scatter(xx, yy, label=k, s=1, c=holds.colour_dict[k[0]])\n",
    "# ax.set_xlabel('X')\n",
    "# ax.set_ylabel('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c82b90",
   "metadata": {},
   "source": [
    "# Human Pose Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fc4069",
   "metadata": {},
   "source": [
    "#  Repeat the Analysis for Each Frames of the submitted video\n",
    "\n",
    "- Get the initial guess of the rendering angle\n",
    "- Sample parameters -> Maximise the number of matching points\n",
    "- Get the full tvec and rvec from the mesh model to the scene\n",
    "- Get the 3D human pose coordinates adjusteds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdeeeb3",
   "metadata": {},
   "source": [
    "SolvePnP for 3D and 2D human pose pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd6acf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_3d2d_solvePnP(humanpose: HumanPose, t: int):\n",
    "    \"\"\"\n",
    "    Obtain Mediappipe 3d pose -> 2d pose projection tvec & tvec at frame t\n",
    "    \"\"\"\n",
    "    rvec, tvec = solvePnP(humanpose.coords3d[t], humanpose.coords2d[t], cameradata)\n",
    "    return rvec, tvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042dc600",
   "metadata": {},
   "outputs": [],
   "source": [
    "rvec_mp2img, tvec_mp2img = mediapipe_3d2d_solvePnP(human_pose, frame_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fd55b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_mp2img = get_extrinsic_matrix(rvec_mp2img, tvec_mp2img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbe37f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose2d_projection, _ = cv2.projectPoints(\n",
    "    human_pose.coords3d[frame_number],\n",
    "    rvec_mp2img, tvec_mp2img,\n",
    "    cameradata.intrinsic_matrix,\n",
    "    np.array(cameradata.distortion_coeff)\n",
    "    )\n",
    "\n",
    "holds2d_projection, _ = cv2.projectPoints(\n",
    "    pts_wld,\n",
    "    rvec_wall, tvec_wall,\n",
    "    cameradata.intrinsic_matrix,\n",
    "    np.array(cameradata.distortion_coeff)\n",
    "    )\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img2)\n",
    "for i in range(pose2d_projection.shape[0]):\n",
    "    ax.scatter(*pose2d_projection[i][0], s=5)\n",
    "\n",
    "for i in range(holds2d_projection.shape[0]):\n",
    "    ax.scatter(*holds2d_projection[i][0], c='white', s=2)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coords_transformation(coords3d, *args):\n",
    "    pose_global = []\n",
    "    for kp in coords3d:\n",
    "        xx, yy, zz = kp\n",
    "        for M in args:\n",
    "            xx, yy, zz, _ = M @ [xx, yy, zz, 1]\n",
    "        # xx, yy, zz, _ = T_wall_inv @ T_mp2img @ [kp[0], kp[1], kp[2], 1]\n",
    "        # xx, yy, zz, _ = T_mp2img @ [kp[0], kp[1], kp[2], 1]\n",
    "        pose_global.append([xx, yy, zz])\n",
    "    return np.array(pose_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821ef9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_global = coords_transformation(human_pose.coords3d[frame_number], T_mp2img, T_wall_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd5b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.set_aspect('equal')\n",
    "# for kp in pose_global:\n",
    "#     ax.scatter(*kp, c='blue', s= 2)\n",
    "\n",
    "body = Body(\n",
    "    frame_number,\n",
    "    human_pose.coords2d[frame_number],\n",
    "    pose_global,\n",
    "    human_pose.coords2d_confidence[frame_number],\n",
    "    human_pose.coords3d_confidence[frame_number],\n",
    "    \"C:/Users/susum/Desktop/1StarChoss_trimmed.pkl\"\n",
    "    )\n",
    "body.plot(ax)\n",
    "\n",
    "for coords in pts_wld:\n",
    "    x, y, z = coords\n",
    "    ax.scatter(x, y, z, c='black', s=3)\n",
    "\n",
    "# for k, v in get_edge_coords_all(pose_global).items():\n",
    "#     p0, p1 = v\n",
    "#     ax.plot([p0[0], p1[0]], [p0[1], p1[1]], [p0[2], p1[2]], c='red')\n",
    "\n",
    "# for k, v in holds.coords_world.items():\n",
    "#     xx, yy, zz, _ = T_wall_rendering @ [v[0], v[1], v[2], 1]\n",
    "#     ax.scatter(xx, yy, zz, label=k, s=1, c=holds.colour_dict[k[0]])\n",
    "\n",
    "# ax.view_init(elev=60, azim=164, roll=-104)\n",
    "# ax.view_init(elev=0, azim=180, roll=-90)\n",
    "# ax.view_init(elev=0, azim=0, roll=90)\n",
    "ax.view_init(elev=90, azim=-90, roll=0)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "# plot_3d(\n",
    "#     plane.plotter(alpha=0.2, lims_x=(-300, 300), lims_y=(-300, 300)),\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac700e9",
   "metadata": {},
   "source": [
    "Get contact info and adjust the coorddinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bf3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "contacts2d = body.contact_points2d\n",
    "contacts3d = body.contact_points3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883fc409",
   "metadata": {},
   "outputs": [],
   "source": [
    "perspective_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed6ca7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords3d_notcalibrated = []\n",
    "coords3d_contact = []\n",
    "\n",
    "fig, axes = plt.subplots(1,2)\n",
    "axes[0].imshow(img1)\n",
    "axes[1].imshow(img2)\n",
    "\n",
    "contacts2d_dict = contacts2d.dict_points\n",
    "contacts3d_dict = contacts3d.dict_points\n",
    "\n",
    "for part in contacts3d.dict_points.keys():\n",
    "    if contacts3d_dict[part]['state'] == 1:\n",
    "        point_to_project = np.array([contacts2d_dict[part]['coords']], dtype='float32').reshape(-1, 1, 2)\n",
    "        projected_point = cv2.perspectiveTransform(point_to_project, np.linalg.inv(perspective_transformation))[0]\n",
    "        axes[0].scatter(*projected_point[0], c='white', s=2)\n",
    "        axes[1].scatter(*point_to_project[0][0], c='white', s=2)\n",
    "\n",
    "        print(\"Projected Point:\", projected_point[0])\n",
    "        p2d, p3d = get_pair_2d_3d(projected_point, point_to_project[0], scn, cam_obj0, K_blender)\n",
    "        print(contacts3d_dict[part]['conf'])\n",
    "        coords3d_notcalibrated.append(contacts3d_dict[part]['coords'])\n",
    "        coords3d_contact.append(p3d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13957463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transformation_matrix(A, B):\n",
    "    # Center the points\n",
    "    A_centered = A - np.mean(A, axis=0)\n",
    "    B_centered = B - np.mean(B, axis=0)\n",
    "\n",
    "    # Compute the covariance matrix\n",
    "    H = np.dot(A_centered.T, B_centered)\n",
    "\n",
    "    # Perform SVD\n",
    "    U, S, Vt = np.linalg.svd(H)\n",
    "    R = np.dot(Vt.T, U.T)\n",
    "\n",
    "    # Ensure a right-handed coordinate system\n",
    "    if np.linalg.det(R) < 0:\n",
    "        Vt[2, :] *= -1\n",
    "        R = np.dot(Vt.T, U.T)\n",
    "\n",
    "    # Compute the translation\n",
    "    t = np.mean(B, axis=0) - np.dot(R, np.mean(A, axis=0))\n",
    "\n",
    "    # Create the transformation matrix\n",
    "    T = np.eye(4)\n",
    "    T[:3, :3] = R\n",
    "    T[:3, 3] = t\n",
    "\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0497874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_to_timestamp(frame_number, fps_video):\n",
    "    time_sec = frame_number / fps_video\n",
    "    minutes = int(time_sec // 60)\n",
    "    seconds = int(time_sec % 60)\n",
    "    milliseconds = int((time_sec * 1000) % 1000)\n",
    "    return f'{minutes:02}:{seconds:02}.{milliseconds:03}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fef9b6",
   "metadata": {},
   "source": [
    "Update the pose 3d coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987451a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the transformation from the current pose to the adjusted pose\n",
    "\n",
    "M = compute_transformation_matrix(coords3d_notcalibrated, coords3d_contact)\n",
    "pose_global_calibrated = coords_transformation(pose_global, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d6571",
   "metadata": {},
   "outputs": [],
   "source": [
    "body = Body(frame_number, human_pose.coords2d[frame_number], pose_global_calibrated, \"C:/Users/susum/Desktop/1StarChoss_trimmed.pkl\")\n",
    "pose2d_projection, _ = cv2.projectPoints(\n",
    "    human_pose.coords3d[frame_number],\n",
    "    rvec_mp2img, tvec_mp2img,\n",
    "    cameradata.intrinsic_matrix,\n",
    "    np.array(cameradata.distortion_coeff)\n",
    "    )\n",
    "\n",
    "holds2d_projection, _ = cv2.projectPoints(\n",
    "    pts_wld,\n",
    "    rvec_wall, tvec_wall,\n",
    "    cameradata.intrinsic_matrix,\n",
    "    np.array(cameradata.distortion_coeff)\n",
    "    )\n",
    "\n",
    "plt.ion()\n",
    "fig = plt.figure(figsize=(13, 10))\n",
    "ax1 = fig.add_subplot(2, 3, 1, projection='3d')\n",
    "ax1.view_init(elev=0, azim=180, roll=-90)\n",
    "ax2 = fig.add_subplot(2, 3, 2, projection='3d')\n",
    "ax2.view_init(elev=0, azim=0, roll=90)\n",
    "ax3 = fig.add_subplot(2, 3, 3, projection='3d')\n",
    "ax3.view_init(elev=90, azim=-90, roll=0)\n",
    "ax4 = fig.add_subplot(2, 3, 4, projection='3d')\n",
    "ax4.view_init(elev=60, azim=164, roll=-104)\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "ax3d = [ax1, ax2, ax3, ax4]\n",
    "ax2d = [ax5, ax6]\n",
    "plt.tight_layout()\n",
    "\n",
    "for ax in ax3d+ax2d:\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "\n",
    "for ax in ax3d:\n",
    "    ax.set_xlim3d(-1.5, 1.5)\n",
    "    ax.set_ylim3d(0, 2.5)\n",
    "    ax.set_zlim3d(0.2, 3.5)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    body.plot(ax, size_contact=30, size_com=30)\n",
    "    for coords in pts_wld:\n",
    "        x, y, z = coords\n",
    "        ax.scatter(x, y, z, c='black', s=3)\n",
    "\n",
    "for ax in ax2d:\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "ax5.imshow(img3)\n",
    "ax6.imshow(img2)\n",
    "for i in range(pose2d_projection.shape[0]):\n",
    "    ax6.scatter(*pose2d_projection[i][0], s=5, c='red', alpha=0.5)\n",
    "\n",
    "for i in range(holds2d_projection.shape[0]):\n",
    "    ax6.scatter(*holds2d_projection[i][0], c='white', s=2)\n",
    "ax1.text2D(0.05, 0.95, f\"{frame_to_timestamp(frame_number, videodata.fps)}\", transform=ax1.transAxes,\n",
    "            fontsize=12, color='red', bbox=dict(facecolor='white', alpha=0.6))\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eca146",
   "metadata": {},
   "source": [
    "# Iterate the routine through the whole video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf19af5",
   "metadata": {},
   "source": [
    "## More work on the rendering angle optimisation for the case camera is moving ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd6eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolomodel = 'models/best.pt'\n",
    "masker = Masker(yolomodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12826de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "videodata_new = VideoData(\"C:/Users/susum/Desktop/1StarChoss_trimmed.mp4\")\n",
    "videodata_copy = VideoData(\"C:/Users/susum/Desktop/1StarChoss_trimmed.mp4\")\n",
    "human_pose = videodata_copy.get_human_pose()\n",
    "del videodata_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e14672",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_frames = videodata.total_frames - 1\n",
    "fps = 40\n",
    "output_path = os.path.join(\n",
    "    os.path.dirname(os.path.abspath('__file__')),\n",
    "    os.pardir,\n",
    "    os.pardir,\n",
    "    'database',\n",
    "    'animation.gif'\n",
    "    )\n",
    "\n",
    "\"\"\"\n",
    "Fig and ax setup\n",
    "\"\"\"\n",
    "plt.ioff()\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "ax1 = fig.add_subplot(2, 3, 1, projection='3d')\n",
    "ax2 = fig.add_subplot(2, 3, 2, projection='3d')\n",
    "ax3 = fig.add_subplot(2, 3, 3, projection='3d')\n",
    "ax4 = fig.add_subplot(2, 3, 4, projection='3d')\n",
    "ax5 = fig.add_subplot(2, 3, 5)\n",
    "ax6 = fig.add_subplot(2, 3, 6)\n",
    "ax3d = [ax1, ax2, ax3, ax4]\n",
    "ax2d = [ax5, ax6]\n",
    "\n",
    "def init_axes():\n",
    "    \"\"\"Initialize static axis properties.\"\"\"\n",
    "    views = [\n",
    "        dict(elev=0, azim=180, roll=-90),\n",
    "        dict(elev=0, azim=0, roll=90),\n",
    "        dict(elev=90, azim=-90, roll=0),\n",
    "        dict(elev=60, azim=164, roll=-104)\n",
    "    ]\n",
    "    for ax, view in zip(ax3d, views):\n",
    "        ax.view_init(**view)\n",
    "        ax.set_xlim3d(-1.5, 1.5)\n",
    "        ax.set_ylim3d(0, 2.5)\n",
    "        ax.set_zlim3d(0.2, 3.5)\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.set_zlabel('Z')\n",
    "        ax.set_aspect('equal')\n",
    "    for ax in ax2d:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_aspect('equal')\n",
    "\n",
    "init_axes()\n",
    "plt.tight_layout()\n",
    "\n",
    "\"\"\"\n",
    "Animation Frame Update Function\n",
    "\"\"\"\n",
    "\n",
    "def update(frame_number):\n",
    "    print(frame_number/videodata.total_frames)\n",
    "    # Clear dynamic content from all axes\n",
    "    for ax in ax3d + ax2d:\n",
    "        ax.cla()\n",
    "    init_axes()  # Reset views and labels\n",
    "\n",
    "    \"\"\"\n",
    "    Actual Computation\n",
    "    \"\"\"\n",
    "\n",
    "    img2 = cv2.undistort(videodata.load_frame(frame_number), cameradata.intrinsic_matrix, cameradata.distortion_coeff)\n",
    "    # Colour -> Grey\n",
    "    img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    # Blurred Grey Images\n",
    "    img2_gray = cv2.GaussianBlur(img2_gray, (3,3), cv2.BORDER_DEFAULT)\n",
    "    img1_adjust = gammaCorrection(img1_gray)\n",
    "    img2_adjust = gammaCorrection(img2_gray)\n",
    "    img1_ = invert(img1_adjust)\n",
    "    img2_ = invert(img2_adjust)\n",
    "\n",
    "    mask = masker.apply_mask(img2, 0.3, cls2include=[0], cls2exclude=[1, 2])\n",
    "    # A-KAZE detector\n",
    "    akaze = cv2.AKAZE_create()\n",
    "    # Feature detection and feature vector computation\n",
    "    # https://docs.opencv.org/4.x/d0/d13/classcv_1_1Feature2D.html#a8be0d1c20b08eb867184b8d74c15a677\n",
    "    kp1, des1 = akaze.detectAndCompute(img1_, None)\n",
    "    kp2, des2 = akaze.detectAndCompute(img2_, mask)\n",
    "\n",
    "    cross_check = True\n",
    "    # cross_check = False\n",
    "\n",
    "    # creating Brute-Force Matcher\n",
    "    # https://docs.opencv.org/4.x/dc/dc3/tutorial_py_matcher.html\n",
    "    if cross_check:\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING2, crossCheck=True)\n",
    "        matches = bf.match(des1, des2)\n",
    "        # Sort them in the order of their distance.\n",
    "        matches_good = sorted(matches, key = lambda x:x.distance)\n",
    "    else:\n",
    "        bf = cv2.BFMatcher()\n",
    "        matches = bf.knnMatch(des1, des2, k=2)\n",
    "        # Select pairs by similarity in feature vectors\n",
    "        matches_good = []\n",
    "        ratio = 0.5\n",
    "        while len(matches_good) < 20:\n",
    "            ratio += 0.01\n",
    "            matches_good = []\n",
    "            for m, n in matches:\n",
    "                if m.distance < ratio * n.distance:\n",
    "                    matches_good.append([m])\n",
    "\n",
    "    # Geometric Verification -> Assuming that the feature points are alighned on the same plane!\n",
    "    # queryIdx: index of the descriptor in the first set\n",
    "    # trainIdx: index of the descriptor in the second set\n",
    "\n",
    "    # Rendered image\n",
    "    pt1 = np.float32([kp1[m.queryIdx].pt for m in matches_good])\n",
    "    # Snapshot\n",
    "    pt2 = np.float32([kp2[m.trainIdx].pt for m in matches_good])\n",
    "\n",
    "    # Compute homography using RANSAC\n",
    "    # https://docs.opencv.org/3.4/d9/d0c/group__calib3d.html#ga4abc2ece9fab9398f2e560d53c8c9780\n",
    "    # https://docs.opencv.org/4.x/d9/dab/tutorial_homography.html\n",
    "    perspective_transformation, mask = cv2.findHomography(pt1, pt2, cv2.RANSAC, 5.0)\n",
    "\n",
    "    matches_plane = []\n",
    "    for i, m in enumerate(matches_good):\n",
    "        if mask[i]:\n",
    "            matches_plane.append(m)\n",
    "\n",
    "    img3 = cv2.drawMatches(img1, kp1, img2, kp2, matches_plane, None, flags=2)\n",
    "\n",
    "    # print('# Point on the Same Plane / # Feature Points: ', len(matches_plane)/len(matches_good))\n",
    "\n",
    "    # Coordinates of the keypoints in reference and target images\n",
    "    pt1 = np.float32([kp1[m.queryIdx].pt for m in matches_plane])\n",
    "    pt2 = np.float32([kp2[m.trainIdx].pt for m in matches_plane])\n",
    "    pts_img, pts_wld = get_pair_2d_3d(pt1, pt2, scn, cam_obj0, K_blender)\n",
    "\n",
    "    rvec_wall, tvec_wall = solvePnP(pts_wld, pts_img, cameradata)\n",
    "\n",
    "    r, theta, height = 8, 0, 1.7\n",
    "    rvec_render = cam_position2rvec(theta)\n",
    "    tvec_render = cam_position2tvec(r, theta, height, vector_fmt=True)\n",
    "    T_wall_rendering = get_extrinsic_matrix(rvec_render, tvec_render)\n",
    "    T_wall_rendering_inv = np.linalg.inv(T_wall_rendering)\n",
    "\n",
    "    T_wall = get_extrinsic_matrix(rvec_wall, tvec_wall)\n",
    "    T_wall_inv = np.linalg.inv(T_wall)\n",
    "\n",
    "    rvec_mp2img, tvec_mp2img = mediapipe_3d2d_solvePnP(human_pose, frame_number)\n",
    "    T_mp2img = get_extrinsic_matrix(rvec_mp2img, tvec_mp2img)\n",
    "\n",
    "    pose_global = coords_transformation(human_pose.coords3d[frame_number], T_mp2img, T_wall_inv)\n",
    "    body = Body(frame_number, human_pose.coords2d[frame_number], pose_global, \"C:/Users/susum/Desktop/1StarChoss_trimmed.pkl\")\n",
    "\n",
    "    contacts2d = body.contact_points2d\n",
    "    contacts3d = body.contact_points3d\n",
    "\n",
    "    coords3d_notcalibrated = []\n",
    "    coords3d_contact = []\n",
    "\n",
    "    contacts2d_dict = contacts2d.dict_points\n",
    "    contacts3d_dict = contacts3d.dict_points\n",
    "\n",
    "    for part in contacts3d.dict_points.keys():\n",
    "        if contacts3d_dict[part]['state'] == 1:\n",
    "            # Projection of the point in video frame to the rendered image\n",
    "            point_to_project = np.array([contacts2d_dict[part]['coords']], dtype='float32').reshape(-1, 1, 2)\n",
    "            projected_point = cv2.perspectiveTransform(point_to_project, np.linalg.inv(perspective_transformation))[0]\n",
    "            p2d, p3d = get_pair_2d_3d(projected_point, point_to_project[0], scn, cam_obj0, K_blender)\n",
    "            \n",
    "            coords3d_notcalibrated.append(contacts3d_dict[part]['coords'])\n",
    "            coords3d_contact.append(p3d[0])\n",
    "    \n",
    "    M = compute_transformation_matrix(coords3d_notcalibrated, coords3d_contact)\n",
    "    pose_global_calibrated = coords_transformation(pose_global, M)\n",
    "\n",
    "    # Visualise the results\n",
    "    body = Body(frame_number, human_pose.coords2d[frame_number], pose_global_calibrated, \"C:/Users/susum/Desktop/1StarChoss_trimmed.pkl\")\n",
    "    pose2d_projection, _ = cv2.projectPoints(\n",
    "        human_pose.coords3d[frame_number],\n",
    "        rvec_mp2img, tvec_mp2img,\n",
    "        cameradata.intrinsic_matrix,\n",
    "        np.array(cameradata.distortion_coeff)\n",
    "        )\n",
    "\n",
    "    holds2d_projection, _ = cv2.projectPoints(\n",
    "        pts_wld,\n",
    "        rvec_wall, tvec_wall,\n",
    "        cameradata.intrinsic_matrix,\n",
    "        np.array(cameradata.distortion_coeff)\n",
    "        )\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Plots Update\n",
    "    \"\"\"\n",
    "\n",
    "    for ax in ax3d:\n",
    "        # body.plot(ax, size_contact=30, size_com=10)\n",
    "        for coords, col in zip(body.edges3d.values(), body.edges_col.values()):\n",
    "            p0, p1 = coords\n",
    "            xs, ys, zs = [p0[0], p1[0]], [p0[1], p1[1]], [p0[2], p1[2]]\n",
    "            ax.plot(xs, ys, zs, c=col)\n",
    "        for point in body.contact_points3d.dict_points.values():\n",
    "            col = 'white'\n",
    "            if point['state'] == 1:\n",
    "                col = 'orange'\n",
    "            x, y, z = point['coords']\n",
    "            ax.scatter(x, y, z, marker='o', c=col, edgecolors='k', s=30)\n",
    "\n",
    "        for key, val in body.dict_com3d.items():\n",
    "            x, y, z = val\n",
    "            ax.scatter(x, y, z, marker='o', c='green', edgecolors='k', s=10)\n",
    "\n",
    "\n",
    "        for coords in pts_wld:\n",
    "            x, y, z = coords\n",
    "            ax.scatter(x, y, z, c='black', s=3, alpha=0.5)\n",
    "    for ax in ax2d:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    ax5.imshow(img3)\n",
    "    ax6.imshow(img2)\n",
    "    for i in range(pose2d_projection.shape[0]):\n",
    "        ax6.scatter(*pose2d_projection[i][0], s=5, c='red', alpha=0.9)\n",
    "\n",
    "    for i in range(holds2d_projection.shape[0]):\n",
    "        ax6.scatter(*holds2d_projection[i][0], c='white', s=2, alpha=0.9)\n",
    "\n",
    "    ax1.text2D(0.05, 0.95, f'{frame_to_timestamp(frame_number, videodata.fps)}', transform=ax1.transAxes,\n",
    "                fontsize=12, color='red', bbox=dict(facecolor='white', alpha=0.6))\n",
    "    return []\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=n_frames, blit=False, interval=1000/fps)\n",
    "ani.save(output_path, writer='ffmpeg', dpi=200)\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fbab1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kakushin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
